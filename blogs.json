{"status":"ok","feed":{"url":"https://medium.com/feed/@nagangapal","title":"Stories by Angadpreet Nagpal on Medium","link":"https://medium.com/@nagangapal?source=rss-e36528fd7102------2","author":"","description":"Stories by Angadpreet Nagpal on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/0*wlaehpqLHH50Ui-1.jpg"},"items":[{"title":"My 3 moths with Computer Vision\u200a\u2014\u200aPart 6\u200a\u2014\u200aUdacity Facial Keypoints\u200a\u2014\u200aUnderstanding the problem","pubDate":"2021-04-20 13:18:29","link":"https://nagangapal.medium.com/my-3-moths-with-computer-vision-part-6-udacity-facial-keypoints-understanding-the-problem-21e00420be48?source=rss-e36528fd7102------2","guid":"https://medium.com/p/21e00420be48","author":"Angadpreet Nagpal","thumbnail":"","description":"\n<h3>My 3 months with Computer Vision\u200a\u2014\u200aPart 6\u200a\u2014\u200aUdacity Facial Keypoints\u200a\u2014\u200aUnderstanding the\u00a0problem</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/930/1*35sG3ORF0g3Qr6VZfQUjXA.png\"><figcaption>Udacity Facial Keypoints</figcaption></figure><p>The data is from <a href=\"https://github.com/udacity/P1_Facial_Keypoints#:~:text=Facial%20keypoints%20include%20points%20around,facial%20filters%2C%20and%20emotion%20recognition.\">Udacity</a> for their <a href=\"https://www.udacity.com/course/computer-vision-nanodegree--nd891\">Computer Vision Nanodegree</a>. The data is scraped from Youtube and contains face and an excel sheet containing the keypoints. The problem asks you to train a neural network which can predict these keypoints.</p>\n<h4>History of the\u00a0problem</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*Sgzr9kr7EQRWAu1G.png\"><figcaption>Facebook tagging\u00a0feature</figcaption></figure><p>Even as back as 2008, Facebook had the tagging feature before neural networks went mainstream. How they did this is using what is know as Haar Cascades. Haar Cascades are xml files which when slid over an image are used for finding nose, eyes, ears and finally using these to detect the face. These have been included in the project <a href=\"https://github.com/udacity/P1_Facial_Keypoints/tree/master/detector_architectures\">here</a>. This is similar to a Convolutional layer but\u00a0static.</p>\n<h3>Neural Networks but slower Using Convolutional Layers</h3>\n<p>One way to do this is to use CNNs the same way we have been doing till now. There are 2 ways we know of for this\u00a0\u2014</p>\n<h4>1. R-CNN</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/850/0*UAQiua5FAW13oFl2.png\"></figure><p>This algorithm consists of 3 main parts\u200a\u2014\u200aregion proposal, Running the algorithm on these regions and then bounding box regression. In short, in the first step, we guess where the objects could be. This can be done using segmentation, using similarity of color. You can also pretrain a Neural Network to guess this. The second step is running convolution on these regions. This gives us the probability of the region that we have predicted has the object. Finally the various bounding boxes are merged together to give us the final bounding\u00a0box.</p>\n<h4>2. Faster\u00a0R-CNN</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/721/0*MH4heX9lnq8BKTaf.jpeg\"></figure><p>Here the region proposal is based on Neural Network and then we repeat the same\u00a0steps.</p>\n<p>We can also optionally divide the image into regions and then run convolution on these regions and then these regions can be\u00a0merged.</p>\n<p>The problem with all these methods is that they are very slow. They cannot be used for real time detection.</p>\n<h3>Come In,\u00a0SSD</h3>\n<p>SSD, not to be confused with solid state drives, are single shot detection algorithm. We basically pass the whole image, no region proposal no nothing. The algorithm gives us an output of points. We then take the sqrt of the difference of proposed and the real points and pass that error back through the neural\u00a0network.</p>\n<p>This Algorithm is also known as YOLO or you only look\u00a0once.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/638/0*KLus9RIzo_kEMwuT\"></figure><p>Please research the above topics in detail if you want but we will be training an SSD Algorithm which is the state of the art for real time object detection at this\u00a0point.</p>\n<h3><strong>Loading the data and seeing how to SSD this Algorithm</strong></h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*e8UMk57WCCWrDXlm.jpg\"><figcaption>Facial Keypoints</figcaption></figure><p>Above are the keypoints we need to predict. A total of 68 points which gives us 136 parameters to predict. The class uses Pytorch, I will be using\u00a0Keras.</p>\n<p>Let\u2019s start by visualizing the keypoints on a\u00a0face.</p>\n<a href=\"https://medium.com/media/c52bd9cd6982083aa1089fcec1686326/href\">https://medium.com/media/c52bd9cd6982083aa1089fcec1686326/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/291/1*BgmaHN5pkp2_Dk1w_KH1uA.png\"><figcaption>Facial Keypoints</figcaption></figure><p>As you can see the key points are placed over the important points like eyes, nose, mouth and face\u00a0corners.</p>\n<p>Now that we know the problem, the history of the problem and how to solve it. Let\u2019s get into the code. The problem will require us to write our own custom Data Augmentation. Let\u2019s start with\u00a0that.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=21e00420be48\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>My 3 months with Computer Vision\u200a\u2014\u200aPart 6\u200a\u2014\u200aUdacity Facial Keypoints\u200a\u2014\u200aUnderstanding the\u00a0problem</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/930/1*35sG3ORF0g3Qr6VZfQUjXA.png\"><figcaption>Udacity Facial Keypoints</figcaption></figure><p>The data is from <a href=\"https://github.com/udacity/P1_Facial_Keypoints#:~:text=Facial%20keypoints%20include%20points%20around,facial%20filters%2C%20and%20emotion%20recognition.\">Udacity</a> for their <a href=\"https://www.udacity.com/course/computer-vision-nanodegree--nd891\">Computer Vision Nanodegree</a>. The data is scraped from Youtube and contains face and an excel sheet containing the keypoints. The problem asks you to train a neural network which can predict these keypoints.</p>\n<h4>History of the\u00a0problem</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*Sgzr9kr7EQRWAu1G.png\"><figcaption>Facebook tagging\u00a0feature</figcaption></figure><p>Even as back as 2008, Facebook had the tagging feature before neural networks went mainstream. How they did this is using what is know as Haar Cascades. Haar Cascades are xml files which when slid over an image are used for finding nose, eyes, ears and finally using these to detect the face. These have been included in the project <a href=\"https://github.com/udacity/P1_Facial_Keypoints/tree/master/detector_architectures\">here</a>. This is similar to a Convolutional layer but\u00a0static.</p>\n<h3>Neural Networks but slower Using Convolutional Layers</h3>\n<p>One way to do this is to use CNNs the same way we have been doing till now. There are 2 ways we know of for this\u00a0\u2014</p>\n<h4>1. R-CNN</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/850/0*UAQiua5FAW13oFl2.png\"></figure><p>This algorithm consists of 3 main parts\u200a\u2014\u200aregion proposal, Running the algorithm on these regions and then bounding box regression. In short, in the first step, we guess where the objects could be. This can be done using segmentation, using similarity of color. You can also pretrain a Neural Network to guess this. The second step is running convolution on these regions. This gives us the probability of the region that we have predicted has the object. Finally the various bounding boxes are merged together to give us the final bounding\u00a0box.</p>\n<h4>2. Faster\u00a0R-CNN</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/721/0*MH4heX9lnq8BKTaf.jpeg\"></figure><p>Here the region proposal is based on Neural Network and then we repeat the same\u00a0steps.</p>\n<p>We can also optionally divide the image into regions and then run convolution on these regions and then these regions can be\u00a0merged.</p>\n<p>The problem with all these methods is that they are very slow. They cannot be used for real time detection.</p>\n<h3>Come In,\u00a0SSD</h3>\n<p>SSD, not to be confused with solid state drives, are single shot detection algorithm. We basically pass the whole image, no region proposal no nothing. The algorithm gives us an output of points. We then take the sqrt of the difference of proposed and the real points and pass that error back through the neural\u00a0network.</p>\n<p>This Algorithm is also known as YOLO or you only look\u00a0once.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/638/0*KLus9RIzo_kEMwuT\"></figure><p>Please research the above topics in detail if you want but we will be training an SSD Algorithm which is the state of the art for real time object detection at this\u00a0point.</p>\n<h3><strong>Loading the data and seeing how to SSD this Algorithm</strong></h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*e8UMk57WCCWrDXlm.jpg\"><figcaption>Facial Keypoints</figcaption></figure><p>Above are the keypoints we need to predict. A total of 68 points which gives us 136 parameters to predict. The class uses Pytorch, I will be using\u00a0Keras.</p>\n<p>Let\u2019s start by visualizing the keypoints on a\u00a0face.</p>\n<a href=\"https://medium.com/media/c52bd9cd6982083aa1089fcec1686326/href\">https://medium.com/media/c52bd9cd6982083aa1089fcec1686326/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/291/1*BgmaHN5pkp2_Dk1w_KH1uA.png\"><figcaption>Facial Keypoints</figcaption></figure><p>As you can see the key points are placed over the important points like eyes, nose, mouth and face\u00a0corners.</p>\n<p>Now that we know the problem, the history of the problem and how to solve it. Let\u2019s get into the code. The problem will require us to write our own custom Data Augmentation. Let\u2019s start with\u00a0that.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=21e00420be48\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["single-shot-detector","data-augmentation","facial-recognition","keras","udacity-nanodegree"]},{"title":"My 3 months with Computer Vision\u200a\u2014\u200aPart 5\u200a\u2014\u200aTransfer Learning for Stanford Dog Dataset","pubDate":"2021-04-18 13:09:04","link":"https://nagangapal.medium.com/my-3-months-with-computer-vision-part-5-transfer-learning-for-stanford-dog-dataset-34ed165f872e?source=rss-e36528fd7102------2","guid":"https://medium.com/p/34ed165f872e","author":"Angadpreet Nagpal","thumbnail":"","description":"\n<h3>My 3 months with Computer Vision\u200a\u2014\u200aPart 5\u200a\u2014\u200aTransfer Learning for Stanford Dog\u00a0Dataset</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*oGeDpOoJQw8jfSCVpijZ7g.jpeg\"><figcaption>Stanford dog\u00a0Dataset</figcaption></figure><p>Let\u2019s start with the 3rd Project\u200a\u2014\u200a<a href=\"https://www.kaggle.com/jessicali9530/stanford-dogs-dataset\">Stanford Dog Dataset</a>. This dataset asks you to identify dogs of 120 different breeds. We can go with our previous approach. Creating a custom neural network for identifying the breed. But that will take a lot of computation and a lot of time. Let\u2019s introduce a new concept\u00a0then.</p>\n<p>You can clone the repository at <a href=\"https://github.com/angadp/DeepLearning\">https://github.com/angadp/DeepLearning</a></p>\n<h4>Transfer Learning</h4>\n<p><a href=\"https://en.wikipedia.org/wiki/Transfer_learning\">Transfer Learning</a> is the concept of using pretrained models for your own use. Most Kaggle competition winners and models in production use transfer learning. These models are trained on dataset of objects and have saved weights for what has worked for recognizing other\u00a0objects.</p>\n<p>We will use frozen transfer learned model ( The features of this model are not learned. ) in the start and then have dense layers to learn the application specific features.</p>\n<a href=\"https://medium.com/media/140439cbba959bbfe7ca5dc4ae0cc838/href\">https://medium.com/media/140439cbba959bbfe7ca5dc4ae0cc838/href</a><p>The above model is called <a href=\"https://arxiv.org/abs/1610.02357\">Xception</a>. We will get the model that was trained on imagenet and the input shape to it will be (300, 300). In the second line, we make all the layers of Xception model to non trainable</p>\n<h3>Loading the\u00a0data</h3>\n<a href=\"https://medium.com/media/64fefcb3d44111283f444fc61352582a/href\">https://medium.com/media/64fefcb3d44111283f444fc61352582a/href</a><p>Similar to how we did it for CatsVDogs, let\u2019s load the data using ImageDataGenerator and flow from directory.</p>\n<h3>Model</h3>\n<p>Let\u2019s now define the model. As we had said, the first layer will be the model we downloaded with the pretrained features and frozen weights and get dense layers to learn application specific features.</p>\n<a href=\"https://medium.com/media/89cea735b69fedec5b98a6a7a79d0d79/href\">https://medium.com/media/89cea735b69fedec5b98a6a7a79d0d79/href</a><p>In the end of the model e have 120 outputs for the 120\u00a0classes.</p>\n<h3>Compile</h3>\n<a href=\"https://medium.com/media/6340616761d69c5d27fdfc4cb2d994e6/href\">https://medium.com/media/6340616761d69c5d27fdfc4cb2d994e6/href</a><p>Again we have categorial as we have 120 categories and we have SGD as optimizer.</p>\n<h3>Training</h3>\n<a href=\"https://medium.com/media/11ee0b4516fa3ef362f06a84b3afd436/href\">https://medium.com/media/11ee0b4516fa3ef362f06a84b3afd436/href</a><p>Finally, we train it. We will get 96% accuracy on training and 90% on validation.</p>\n<h3><strong>Conclusion</strong></h3>\n<p>For larger models, it is better to go for pretrained models with frozen weights. Otherwise the amount of computation and time required is\u00a0large.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=34ed165f872e\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>My 3 months with Computer Vision\u200a\u2014\u200aPart 5\u200a\u2014\u200aTransfer Learning for Stanford Dog\u00a0Dataset</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*oGeDpOoJQw8jfSCVpijZ7g.jpeg\"><figcaption>Stanford dog\u00a0Dataset</figcaption></figure><p>Let\u2019s start with the 3rd Project\u200a\u2014\u200a<a href=\"https://www.kaggle.com/jessicali9530/stanford-dogs-dataset\">Stanford Dog Dataset</a>. This dataset asks you to identify dogs of 120 different breeds. We can go with our previous approach. Creating a custom neural network for identifying the breed. But that will take a lot of computation and a lot of time. Let\u2019s introduce a new concept\u00a0then.</p>\n<p>You can clone the repository at <a href=\"https://github.com/angadp/DeepLearning\">https://github.com/angadp/DeepLearning</a></p>\n<h4>Transfer Learning</h4>\n<p><a href=\"https://en.wikipedia.org/wiki/Transfer_learning\">Transfer Learning</a> is the concept of using pretrained models for your own use. Most Kaggle competition winners and models in production use transfer learning. These models are trained on dataset of objects and have saved weights for what has worked for recognizing other\u00a0objects.</p>\n<p>We will use frozen transfer learned model ( The features of this model are not learned. ) in the start and then have dense layers to learn the application specific features.</p>\n<a href=\"https://medium.com/media/140439cbba959bbfe7ca5dc4ae0cc838/href\">https://medium.com/media/140439cbba959bbfe7ca5dc4ae0cc838/href</a><p>The above model is called <a href=\"https://arxiv.org/abs/1610.02357\">Xception</a>. We will get the model that was trained on imagenet and the input shape to it will be (300, 300). In the second line, we make all the layers of Xception model to non trainable</p>\n<h3>Loading the\u00a0data</h3>\n<a href=\"https://medium.com/media/64fefcb3d44111283f444fc61352582a/href\">https://medium.com/media/64fefcb3d44111283f444fc61352582a/href</a><p>Similar to how we did it for CatsVDogs, let\u2019s load the data using ImageDataGenerator and flow from directory.</p>\n<h3>Model</h3>\n<p>Let\u2019s now define the model. As we had said, the first layer will be the model we downloaded with the pretrained features and frozen weights and get dense layers to learn application specific features.</p>\n<a href=\"https://medium.com/media/89cea735b69fedec5b98a6a7a79d0d79/href\">https://medium.com/media/89cea735b69fedec5b98a6a7a79d0d79/href</a><p>In the end of the model e have 120 outputs for the 120\u00a0classes.</p>\n<h3>Compile</h3>\n<a href=\"https://medium.com/media/6340616761d69c5d27fdfc4cb2d994e6/href\">https://medium.com/media/6340616761d69c5d27fdfc4cb2d994e6/href</a><p>Again we have categorial as we have 120 categories and we have SGD as optimizer.</p>\n<h3>Training</h3>\n<a href=\"https://medium.com/media/11ee0b4516fa3ef362f06a84b3afd436/href\">https://medium.com/media/11ee0b4516fa3ef362f06a84b3afd436/href</a><p>Finally, we train it. We will get 96% accuracy on training and 90% on validation.</p>\n<h3><strong>Conclusion</strong></h3>\n<p>For larger models, it is better to go for pretrained models with frozen weights. Otherwise the amount of computation and time required is\u00a0large.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=34ed165f872e\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["data-augmentation","keras","kaggle","deep-learning","transfer-learning"]},{"title":"My 3 Months with Computer Vision\u200a\u2014\u200aPart 4\u200a\u2014\u200aPredicting CatsVDog\u200a\u2014\u200aMNIST but with bigger images and\u2026","pubDate":"2021-04-18 09:18:06","link":"https://nagangapal.medium.com/my-3-months-with-computer-vision-part-4-predicting-catsvdog-mnist-but-with-bigger-images-and-f403f35be927?source=rss-e36528fd7102------2","guid":"https://medium.com/p/f403f35be927","author":"Angadpreet Nagpal","thumbnail":"","description":"\n<h3>My 3 Months with Computer Vision\u200a\u2014\u200aPart 4\u200a\u2014\u200aPredicting CatsVDog\u200a\u2014\u200aMNIST but with bigger images and less\u00a0labels</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/480/1*dua0qFvL01h0IjDcGx_n9A.gif\"><figcaption>Cats V\u00a0Dogs</figcaption></figure><p>Let\u2019s start with the next project\u200a\u2014\u200aCats Vs Dogs. The data is from <a href=\"https://www.kaggle.com/c/dogs-vs-cats/data\">https://www.kaggle.com/c/dogs-vs-cats/data</a>. This project gives you images of cats and dogs and asks you to predict whether it is cat or\u00a0dog.</p>\n<h3>1. Loading and Augmenting the\u00a0Data</h3>\n<p>You can load the data either from <a href=\"https://www.kaggle.com/c/dogs-vs-cats/data\">https://www.kaggle.com/c/dogs-vs-cats/data</a> or by cloning <a href=\"https://github.com/angadp/DeepLearning\">https://github.com/angadp/DeepLearning</a></p>\n<p>Let\u2019s start loading the data. We will be using a new function for this flow_from_directory. This will open he directory, copy the images and augment it within the same function.</p>\n<a href=\"https://medium.com/media/95feff6fff63d17c0b1be100cccc644c/href\">https://medium.com/media/95feff6fff63d17c0b1be100cccc644c/href</a><p>You start by defining the ImageDataGenerator as we had done before for MNIST and in the end, <a href=\"https://keras.io/api/preprocessing/image/\">flow the images from directory</a>. Flow from directory will change the image size to target size put it into batches and create the output\u00a0labels.</p>\n<p>This is the only thing different from the thing we did in MNIST. Now we can define a model and train\u00a0it.</p>\n<h3>2. Model</h3>\n<a href=\"https://medium.com/media/55a50e981ad248e00e8a88b6861234ac/href\">https://medium.com/media/55a50e981ad248e00e8a88b6861234ac/href</a><pre>Model: \"sequential\"<br>_________________________________________________________________<br>Layer (type)                 Output Shape              Param #   <br>=================================================================<br>conv2d (Conv2D)              (None, 110, 110, 256)     93184     <br>_________________________________________________________________<br>max_pooling2d (MaxPooling2D) (None, 55, 55, 256)       0         <br>_________________________________________________________________<br>conv2d_1 (Conv2D)            (None, 51, 51, 128)       819328    <br>_________________________________________________________________<br>max_pooling2d_1 (MaxPooling2 (None, 25, 25, 128)       0         <br>_________________________________________________________________<br>batch_normalization (BatchNo (None, 25, 25, 128)       512       <br>_________________________________________________________________<br>conv2d_2 (Conv2D)            (None, 23, 23, 96)        110688    <br>_________________________________________________________________<br>conv2d_3 (Conv2D)            (None, 21, 21, 64)        55360     <br>_________________________________________________________________<br>max_pooling2d_2 (MaxPooling2 (None, 10, 10, 64)        0         <br>_________________________________________________________________<br>dropout (Dropout)            (None, 10, 10, 64)        0         <br>_________________________________________________________________<br>conv2d_4 (Conv2D)            (None, 8, 8, 16)          9232      <br>_________________________________________________________________<br>max_pooling2d_3 (MaxPooling2 (None, 4, 4, 16)          0         <br>_________________________________________________________________<br>flatten (Flatten)            (None, 256)               0         <br>_________________________________________________________________<br>dense (Dense)                (None, 128)               32896     <br>_________________________________________________________________<br>dropout_1 (Dropout)          (None, 128)               0         <br>_________________________________________________________________<br>dense_1 (Dense)              (None, 32)                4128      <br>_________________________________________________________________<br>dense_2 (Dense)              (None, 1)                 33        <br>=================================================================<br>Total params: 1,125,361<br>Trainable params: 1,125,105<br>Non-trainable params: 256<br>_________________________________________________________________</pre>\n<p>In this one, I would recommend you create your own model. This will give the whole view of how models are created for different datasets. The only thing different here is the last layer has sigmoid and the model is larger and we have create more layers. There is a problem we encounter when we go for larger and larger model, it is known as <a href=\"https://en.wikipedia.org/wiki/Vanishing_gradient_problem\">vanishing gradient problem</a>. Basically, the amount of loss goes down as we the number of layers increases.</p>\n<h3>3. Compile</h3>\n<p>In this one we will have to use binary_crossentropy as there are only 2 classes. Plus the optimizer is SGD. As I said, all machine learning is trying out different parameters to get the\u00a0result.</p>\n<a href=\"https://medium.com/media/88dfc166d85cf0e9a63b915773201d29/href\">https://medium.com/media/88dfc166d85cf0e9a63b915773201d29/href</a><h3>4. Train</h3>\n<a href=\"https://medium.com/media/5125893b6d8499c2ae6277e2db10c12f/href\">https://medium.com/media/5125893b6d8499c2ae6277e2db10c12f/href</a><p>Let\u2019s explore callback at this point. Callback can be used to stop the training early based on whether the training has reached an inflection point, like the loss hasn\u2019t changed for a while or the accuracy hasn\u2019t changed for a\u00a0while.</p>\n<a href=\"https://medium.com/media/c292b9b121fe573de4a2366b6831fee4/href\">https://medium.com/media/c292b9b121fe573de4a2366b6831fee4/href</a><p>Again, you can repeat the same steps as before, the purpose of this article is to solve a similar problem to MNIST but\u00a0larger.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f403f35be927\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>My 3 Months with Computer Vision\u200a\u2014\u200aPart 4\u200a\u2014\u200aPredicting CatsVDog\u200a\u2014\u200aMNIST but with bigger images and less\u00a0labels</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/480/1*dua0qFvL01h0IjDcGx_n9A.gif\"><figcaption>Cats V\u00a0Dogs</figcaption></figure><p>Let\u2019s start with the next project\u200a\u2014\u200aCats Vs Dogs. The data is from <a href=\"https://www.kaggle.com/c/dogs-vs-cats/data\">https://www.kaggle.com/c/dogs-vs-cats/data</a>. This project gives you images of cats and dogs and asks you to predict whether it is cat or\u00a0dog.</p>\n<h3>1. Loading and Augmenting the\u00a0Data</h3>\n<p>You can load the data either from <a href=\"https://www.kaggle.com/c/dogs-vs-cats/data\">https://www.kaggle.com/c/dogs-vs-cats/data</a> or by cloning <a href=\"https://github.com/angadp/DeepLearning\">https://github.com/angadp/DeepLearning</a></p>\n<p>Let\u2019s start loading the data. We will be using a new function for this flow_from_directory. This will open he directory, copy the images and augment it within the same function.</p>\n<a href=\"https://medium.com/media/95feff6fff63d17c0b1be100cccc644c/href\">https://medium.com/media/95feff6fff63d17c0b1be100cccc644c/href</a><p>You start by defining the ImageDataGenerator as we had done before for MNIST and in the end, <a href=\"https://keras.io/api/preprocessing/image/\">flow the images from directory</a>. Flow from directory will change the image size to target size put it into batches and create the output\u00a0labels.</p>\n<p>This is the only thing different from the thing we did in MNIST. Now we can define a model and train\u00a0it.</p>\n<h3>2. Model</h3>\n<a href=\"https://medium.com/media/55a50e981ad248e00e8a88b6861234ac/href\">https://medium.com/media/55a50e981ad248e00e8a88b6861234ac/href</a><pre>Model: \"sequential\"<br>_________________________________________________________________<br>Layer (type)                 Output Shape              Param #   <br>=================================================================<br>conv2d (Conv2D)              (None, 110, 110, 256)     93184     <br>_________________________________________________________________<br>max_pooling2d (MaxPooling2D) (None, 55, 55, 256)       0         <br>_________________________________________________________________<br>conv2d_1 (Conv2D)            (None, 51, 51, 128)       819328    <br>_________________________________________________________________<br>max_pooling2d_1 (MaxPooling2 (None, 25, 25, 128)       0         <br>_________________________________________________________________<br>batch_normalization (BatchNo (None, 25, 25, 128)       512       <br>_________________________________________________________________<br>conv2d_2 (Conv2D)            (None, 23, 23, 96)        110688    <br>_________________________________________________________________<br>conv2d_3 (Conv2D)            (None, 21, 21, 64)        55360     <br>_________________________________________________________________<br>max_pooling2d_2 (MaxPooling2 (None, 10, 10, 64)        0         <br>_________________________________________________________________<br>dropout (Dropout)            (None, 10, 10, 64)        0         <br>_________________________________________________________________<br>conv2d_4 (Conv2D)            (None, 8, 8, 16)          9232      <br>_________________________________________________________________<br>max_pooling2d_3 (MaxPooling2 (None, 4, 4, 16)          0         <br>_________________________________________________________________<br>flatten (Flatten)            (None, 256)               0         <br>_________________________________________________________________<br>dense (Dense)                (None, 128)               32896     <br>_________________________________________________________________<br>dropout_1 (Dropout)          (None, 128)               0         <br>_________________________________________________________________<br>dense_1 (Dense)              (None, 32)                4128      <br>_________________________________________________________________<br>dense_2 (Dense)              (None, 1)                 33        <br>=================================================================<br>Total params: 1,125,361<br>Trainable params: 1,125,105<br>Non-trainable params: 256<br>_________________________________________________________________</pre>\n<p>In this one, I would recommend you create your own model. This will give the whole view of how models are created for different datasets. The only thing different here is the last layer has sigmoid and the model is larger and we have create more layers. There is a problem we encounter when we go for larger and larger model, it is known as <a href=\"https://en.wikipedia.org/wiki/Vanishing_gradient_problem\">vanishing gradient problem</a>. Basically, the amount of loss goes down as we the number of layers increases.</p>\n<h3>3. Compile</h3>\n<p>In this one we will have to use binary_crossentropy as there are only 2 classes. Plus the optimizer is SGD. As I said, all machine learning is trying out different parameters to get the\u00a0result.</p>\n<a href=\"https://medium.com/media/88dfc166d85cf0e9a63b915773201d29/href\">https://medium.com/media/88dfc166d85cf0e9a63b915773201d29/href</a><h3>4. Train</h3>\n<a href=\"https://medium.com/media/5125893b6d8499c2ae6277e2db10c12f/href\">https://medium.com/media/5125893b6d8499c2ae6277e2db10c12f/href</a><p>Let\u2019s explore callback at this point. Callback can be used to stop the training early based on whether the training has reached an inflection point, like the loss hasn\u2019t changed for a while or the accuracy hasn\u2019t changed for a\u00a0while.</p>\n<a href=\"https://medium.com/media/c292b9b121fe573de4a2366b6831fee4/href\">https://medium.com/media/c292b9b121fe573de4a2366b6831fee4/href</a><p>Again, you can repeat the same steps as before, the purpose of this article is to solve a similar problem to MNIST but\u00a0larger.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f403f35be927\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["data-augmentation","keras","cats-and-dogs","deep-learning"]},{"title":"My 3 months with Computer Vision\u200a\u2014\u200aPart 3\u2014 Simple Neural Network from scratch for MNIST","pubDate":"2021-04-18 07:20:25","link":"https://nagangapal.medium.com/my-3-months-with-computer-vision-part-3-simple-neural-network-from-scratch-for-mnist-fa0035daac98?source=rss-e36528fd7102------2","guid":"https://medium.com/p/fa0035daac98","author":"Angadpreet Nagpal","thumbnail":"","description":"\n<h3>My 3 months with Computer Vision\u200a\u2014\u200aPart 3\u2014 Simple Neural Network from scratch for\u00a0MNIST</h3>\n<p>Let\u2019s start with Project 1\u200a\u2014\u200aMNIST dataset. <a href=\"https://en.wikipedia.org/wiki/MNIST_database\">MNIST</a> is dataset of handwritten digits. These are 28, 28 pixel images and you need to predict what digit it is between 0 and\u00a09.</p>\n<h3>1. Loading The\u00a0Data</h3>\n<p>Let\u2019s start by downloading the data. The data is from Kaggle <a href=\"https://www.kaggle.com/c/digit-recognizer\">https://www.kaggle.com/c/digit-recognizer</a>\u00a0.We will also create a submission for Kaggle and see how our model performs.</p>\n<p>You can start by cloning my repository <a href=\"https://github.com/angadp/DeepLearning\">https://github.com/angadp/DeepLearning</a></p>\n<p>Let\u2019s load the data and see a few\u00a0Samples:</p>\n<a href=\"https://medium.com/media/2115f71a6dade7ca8fa1fa443e425f08/href\">https://medium.com/media/2115f71a6dade7ca8fa1fa443e425f08/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/proxy/1*9zZ5_GE_kwZGZa5qPujqOg.png\"><figcaption>MNIST visualized</figcaption></figure><h3><strong>2. Augmenting the\u00a0data</strong></h3>\n<p>We will use the <a href=\"https://keras.io/api/preprocessing/image/\">ImageDataGenerator</a> function for this. Basically you pass images to this function and it gives you the same images but with the images augmented. This is done to make sure that the right features are learnt. You are basically learning the same images but in a different way.</p>\n<a href=\"https://medium.com/media/b66415d8ec9671133031c81a51e7e2b6/href\">https://medium.com/media/b66415d8ec9671133031c81a51e7e2b6/href</a><h3><strong>3. Model</strong></h3>\n<p>Let\u2019s go to the model now. If you have read the previous article, we need to create a convolution i.e. a model that starts big and becomes\u00a0small.</p>\n<a href=\"https://medium.com/media/449566c666182a0dbc4e8eca079adb3c/href\">https://medium.com/media/449566c666182a0dbc4e8eca079adb3c/href</a><p>Please go through the Keras documentation, you can experiment with more perimeters of this Con2D Layers and MaxPooling Layers. Just make sure that the no of parameters are decreasing every layer. If you visualize this model you will see what I\u00a0mean,</p>\n<pre>Model: \"sequential\"<br>_________________________________________________________________<br>Layer (type)                 Output Shape              Param #   <br>=================================================================<br>conv2d (Conv2D)              (None, 26, 26, 256)       2560      <br>_________________________________________________________________<br>max_pooling2d (MaxPooling2D) (None, 13, 13, 256)       0         <br>_________________________________________________________________<br>conv2d_1 (Conv2D)            (None, 11, 11, 128)       295040    <br>_________________________________________________________________<br>batch_normalization (BatchNo (None, 11, 11, 128)       512       <br>_________________________________________________________________<br>conv2d_2 (Conv2D)            (None, 9, 9, 96)          110688    <br>_________________________________________________________________<br>conv2d_3 (Conv2D)            (None, 7, 7, 64)          55360     <br>_________________________________________________________________<br>max_pooling2d_1 (MaxPooling2 (None, 3, 3, 64)          0         <br>_________________________________________________________________<br>dropout (Dropout)            (None, 3, 3, 64)          0         <br>_________________________________________________________________<br>flatten (Flatten)            (None, 576)               0         <br>_________________________________________________________________<br>dense (Dense)                (None, 128)               73856     <br>_________________________________________________________________<br>dropout_1 (Dropout)          (None, 128)               0         <br>_________________________________________________________________<br>dense_1 (Dense)              (None, 32)                4128      <br>_________________________________________________________________<br>dense_2 (Dense)              (None, 10)                330       <br>=================================================================<br>Total params: 542,474<br>Trainable params: 542,218<br>Non-trainable params: 256<br>_________________________________________________________________</pre>\n<p>As you can see, the number of parameters are decreasing as we go from starting layer to another. Now the work will be done by the training\u00a0method.</p>\n<h3>4. Compile</h3>\n<p>We are predicting categories, so the loss is cateogorial_crossentropy.</p>\n<a href=\"https://medium.com/media/65fa00c568036a0825860fea55798de2/href\">https://medium.com/media/65fa00c568036a0825860fea55798de2/href</a><h3>5. Training and Visualizing</h3>\n<a href=\"https://medium.com/media/3089be06859fde21b10462ab415cb0d4/href\">https://medium.com/media/3089be06859fde21b10462ab415cb0d4/href</a><p>You define the batch size and epochs and the rest is taken care by\u00a0Keras.</p>\n<p>After training we will see a graph like\u00a0this.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/378/1*N2eIuonz5wACKzfc3h12XQ.png\"></figure><h3>6. Predicting</h3>\n<p>You can use the model you have trained to predict new items as shown\u00a0below.</p>\n<a href=\"https://medium.com/media/3e88cbad4bf64996d5793a4f1ad297d5/href\">https://medium.com/media/3e88cbad4bf64996d5793a4f1ad297d5/href</a><p>Next let\u2019s move on <a href=\"https://nagangapal.medium.com/my-3-months-with-computer-vision-part-4-predicting-catsvdog-mnist-but-with-bigger-images-and-f403f35be927\">CatsVDogs</a>.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=fa0035daac98\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>My 3 months with Computer Vision\u200a\u2014\u200aPart 3\u2014 Simple Neural Network from scratch for\u00a0MNIST</h3>\n<p>Let\u2019s start with Project 1\u200a\u2014\u200aMNIST dataset. <a href=\"https://en.wikipedia.org/wiki/MNIST_database\">MNIST</a> is dataset of handwritten digits. These are 28, 28 pixel images and you need to predict what digit it is between 0 and\u00a09.</p>\n<h3>1. Loading The\u00a0Data</h3>\n<p>Let\u2019s start by downloading the data. The data is from Kaggle <a href=\"https://www.kaggle.com/c/digit-recognizer\">https://www.kaggle.com/c/digit-recognizer</a>\u00a0.We will also create a submission for Kaggle and see how our model performs.</p>\n<p>You can start by cloning my repository <a href=\"https://github.com/angadp/DeepLearning\">https://github.com/angadp/DeepLearning</a></p>\n<p>Let\u2019s load the data and see a few\u00a0Samples:</p>\n<a href=\"https://medium.com/media/2115f71a6dade7ca8fa1fa443e425f08/href\">https://medium.com/media/2115f71a6dade7ca8fa1fa443e425f08/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/proxy/1*9zZ5_GE_kwZGZa5qPujqOg.png\"><figcaption>MNIST visualized</figcaption></figure><h3><strong>2. Augmenting the\u00a0data</strong></h3>\n<p>We will use the <a href=\"https://keras.io/api/preprocessing/image/\">ImageDataGenerator</a> function for this. Basically you pass images to this function and it gives you the same images but with the images augmented. This is done to make sure that the right features are learnt. You are basically learning the same images but in a different way.</p>\n<a href=\"https://medium.com/media/b66415d8ec9671133031c81a51e7e2b6/href\">https://medium.com/media/b66415d8ec9671133031c81a51e7e2b6/href</a><h3><strong>3. Model</strong></h3>\n<p>Let\u2019s go to the model now. If you have read the previous article, we need to create a convolution i.e. a model that starts big and becomes\u00a0small.</p>\n<a href=\"https://medium.com/media/449566c666182a0dbc4e8eca079adb3c/href\">https://medium.com/media/449566c666182a0dbc4e8eca079adb3c/href</a><p>Please go through the Keras documentation, you can experiment with more perimeters of this Con2D Layers and MaxPooling Layers. Just make sure that the no of parameters are decreasing every layer. If you visualize this model you will see what I\u00a0mean,</p>\n<pre>Model: \"sequential\"<br>_________________________________________________________________<br>Layer (type)                 Output Shape              Param #   <br>=================================================================<br>conv2d (Conv2D)              (None, 26, 26, 256)       2560      <br>_________________________________________________________________<br>max_pooling2d (MaxPooling2D) (None, 13, 13, 256)       0         <br>_________________________________________________________________<br>conv2d_1 (Conv2D)            (None, 11, 11, 128)       295040    <br>_________________________________________________________________<br>batch_normalization (BatchNo (None, 11, 11, 128)       512       <br>_________________________________________________________________<br>conv2d_2 (Conv2D)            (None, 9, 9, 96)          110688    <br>_________________________________________________________________<br>conv2d_3 (Conv2D)            (None, 7, 7, 64)          55360     <br>_________________________________________________________________<br>max_pooling2d_1 (MaxPooling2 (None, 3, 3, 64)          0         <br>_________________________________________________________________<br>dropout (Dropout)            (None, 3, 3, 64)          0         <br>_________________________________________________________________<br>flatten (Flatten)            (None, 576)               0         <br>_________________________________________________________________<br>dense (Dense)                (None, 128)               73856     <br>_________________________________________________________________<br>dropout_1 (Dropout)          (None, 128)               0         <br>_________________________________________________________________<br>dense_1 (Dense)              (None, 32)                4128      <br>_________________________________________________________________<br>dense_2 (Dense)              (None, 10)                330       <br>=================================================================<br>Total params: 542,474<br>Trainable params: 542,218<br>Non-trainable params: 256<br>_________________________________________________________________</pre>\n<p>As you can see, the number of parameters are decreasing as we go from starting layer to another. Now the work will be done by the training\u00a0method.</p>\n<h3>4. Compile</h3>\n<p>We are predicting categories, so the loss is cateogorial_crossentropy.</p>\n<a href=\"https://medium.com/media/65fa00c568036a0825860fea55798de2/href\">https://medium.com/media/65fa00c568036a0825860fea55798de2/href</a><h3>5. Training and Visualizing</h3>\n<a href=\"https://medium.com/media/3089be06859fde21b10462ab415cb0d4/href\">https://medium.com/media/3089be06859fde21b10462ab415cb0d4/href</a><p>You define the batch size and epochs and the rest is taken care by\u00a0Keras.</p>\n<p>After training we will see a graph like\u00a0this.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/378/1*N2eIuonz5wACKzfc3h12XQ.png\"></figure><h3>6. Predicting</h3>\n<p>You can use the model you have trained to predict new items as shown\u00a0below.</p>\n<a href=\"https://medium.com/media/3e88cbad4bf64996d5793a4f1ad297d5/href\">https://medium.com/media/3e88cbad4bf64996d5793a4f1ad297d5/href</a><p>Next let\u2019s move on <a href=\"https://nagangapal.medium.com/my-3-months-with-computer-vision-part-4-predicting-catsvdog-mnist-but-with-bigger-images-and-f403f35be927\">CatsVDogs</a>.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=fa0035daac98\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["keras","deep-learning","mnist-dataset","data-augmentation"]},{"title":"My Journey into Computer Vision\u200a\u2014\u200aPart 1","pubDate":"2021-04-04 10:39:53","link":"https://nagangapal.medium.com/my-journey-into-computer-vision-part-1-6d81eddb265c?source=rss-e36528fd7102------2","guid":"https://medium.com/p/6d81eddb265c","author":"Angadpreet Nagpal","thumbnail":"","description":"\n<h3>My 3 months with Computer Vision\u200a\u2014\u200aPart\u00a01</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/360/1*IzrmNHH7-HwdvQkzmToFYQ.png\"><figcaption>Computer Vision</figcaption></figure><p>Recently, I started looking into Computer Vision and how to approach these kind of projects. In the past 4 months, I have gone through Coursera, Udacity and Udemy courses. Here is the summary of how to approach any Computer Vision Project\u00a0-</p>\n<ol>\n<li>Do EDA to understand what is to be predicted.</li>\n<li>Decide on the model to run: we will run 2 projects with our custom model and 2 with Transfer\u00a0Learning</li>\n<li>Do data augmentation to generalize your model to work on unseen\u00a0data.</li>\n<li>Finally, evaluate your model, freeze it and\u00a0deploy.</li>\n</ol>\n<a href=\"https://medium.com/media/2206e0bcdf4d61dc3828a9745b52f5a2/href\">https://medium.com/media/2206e0bcdf4d61dc3828a9745b52f5a2/href</a><p>All projects are done in Keras. We will be doing 4 projects:</p>\n<ol>\n<li>MNIST\u00a0: In the MNIST we will work with (28, 28) images and custom model to get an idea of the different layers there are in\u00a0Keras.</li>\n<li>Dog Vs Cat: In this we will be differentiating between images of cats and dogs. We will use custom model again and show you don\u2019t need Transfer Learning for smaller learnings.</li>\n<li>Stanford Dog Breed: This is dataset from Stanford. It contains images of dogs of 120 different breeds. We will use transfer learning for\u00a0this.</li>\n<li>Udacity Facial Keypoints: This is Project 1 from Udacity in their computer vision Course. We will again use Transfer Learning. Along with this, we will also be writing our own data augmentation generator.</li>\n</ol>\n<p>Let\u2019s start by understanding Neural Networks.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6d81eddb265c\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>My 3 months with Computer Vision\u200a\u2014\u200aPart\u00a01</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/360/1*IzrmNHH7-HwdvQkzmToFYQ.png\"><figcaption>Computer Vision</figcaption></figure><p>Recently, I started looking into Computer Vision and how to approach these kind of projects. In the past 4 months, I have gone through Coursera, Udacity and Udemy courses. Here is the summary of how to approach any Computer Vision Project\u00a0-</p>\n<ol>\n<li>Do EDA to understand what is to be predicted.</li>\n<li>Decide on the model to run: we will run 2 projects with our custom model and 2 with Transfer\u00a0Learning</li>\n<li>Do data augmentation to generalize your model to work on unseen\u00a0data.</li>\n<li>Finally, evaluate your model, freeze it and\u00a0deploy.</li>\n</ol>\n<a href=\"https://medium.com/media/2206e0bcdf4d61dc3828a9745b52f5a2/href\">https://medium.com/media/2206e0bcdf4d61dc3828a9745b52f5a2/href</a><p>All projects are done in Keras. We will be doing 4 projects:</p>\n<ol>\n<li>MNIST\u00a0: In the MNIST we will work with (28, 28) images and custom model to get an idea of the different layers there are in\u00a0Keras.</li>\n<li>Dog Vs Cat: In this we will be differentiating between images of cats and dogs. We will use custom model again and show you don\u2019t need Transfer Learning for smaller learnings.</li>\n<li>Stanford Dog Breed: This is dataset from Stanford. It contains images of dogs of 120 different breeds. We will use transfer learning for\u00a0this.</li>\n<li>Udacity Facial Keypoints: This is Project 1 from Udacity in their computer vision Course. We will again use Transfer Learning. Along with this, we will also be writing our own data augmentation generator.</li>\n</ol>\n<p>Let\u2019s start by understanding Neural Networks.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6d81eddb265c\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["data-augmentation","tensorflow","computer-vision","keras","deep-learning"]},{"title":"My Journey into Computer Vision\u200a\u2014\u200aPart 2","pubDate":"2021-04-02 13:58:50","link":"https://nagangapal.medium.com/my-journey-into-computer-vision-part-2-8e69489c860d?source=rss-e36528fd7102------2","guid":"https://medium.com/p/8e69489c860d","author":"Angadpreet Nagpal","thumbnail":"","description":"\n<h3>My 3 months with Computer Vision\u200a\u2014\u200aPart 2\u200a\u2014\u200aUnderstanding Neural Network and\u00a0Layers</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/236/1*Zz19MwrI4-gzYNoEoxJvnQ.png\"><figcaption>Neural Network</figcaption></figure><h3><strong>What are Neural Networks?</strong></h3>\n<p><a href=\"https://en.wikipedia.org/wiki/Neural_network\">Neural Network</a> in very simple terms is a bunch of neural which perform transformations from one variable to other. These nodes are arranged together in the hidden layer. The values flow from the input to the output and the error that comes is then <a href=\"https://en.wikipedia.org/wiki/Backpropagation\">backpropagated</a> to the input layers. This is known as the Dense Layer and is the foundation of Neural Networks. So we already have 2 important parameters to understand for training a neural network\u00a0\u2014</p>\n<ol>\n<li>Error</li>\n<li>And how is this error backpropagated?</li>\n</ol>\n<h3><strong>What is Deep Learning?</strong></h3>\n<p>Deep Learning is just adding some layers before the dense layers which try to detect the features which can be trained. For example, before these layers were introduced, Software Engineers used to have to program features into the neural network. Now these can be auto detected using the following layers.</p>\n<h4><strong>Understanding Convolution</strong></h4>\n<p>Convolution means that the input starts large and multi dimensional and is reduced to smaller output. So you start by adding Convolutional layers and you keep reducing the size of the input to be processed every\u00a0layer.</p>\n<h4><strong>Computer Vision\u00a0Layers</strong></h4>\n<p><strong>1.1 Conv2D</strong></p>\n<p><a href=\"https://keras.io/api/layers/convolution_layers/convolution2d/\">Conv2D</a> is a list of filters which you pass over the image or the image array and that filtered output is now the input to the next layer. These filters are slid over the array and multiplied with the layer below. You can read about the parameters you can pass in the <a href=\"https://keras.io/api/layers/convolution_layers/convolution2d/\">Keras</a> documentation.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/500/0*JS_gxiQjnpPQvIaV.gif\"><figcaption>Convolution Visualized</figcaption></figure><p><strong>1.2 MaxPooling2D</strong></p>\n<p><a href=\"https://keras.io/api/layers/pooling_layers/max_pooling2d/\">MaxPooling2D</a> is a pooling layer and is much simpler to explain. All this does is given height and width, find the max value from that array. The filter is slid over the beneath layer just like\u00a0before.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/728/0*ETjRQFIb45O-MLw4.gif\"><figcaption>MaxPooling2D Visualized</figcaption></figure><p><strong>1.3 Normalizing &amp; Regularization Layers</strong></p>\n<p>We need these layers to keep the model robust to change. The model might get into a wrong tangent and think that some parameter is really important. <a href=\"https://keras.io/api/layers/regularization_layers/dropout/\">Dropout</a> drops some nodes at random. This makes sure that the model is learning all nodes equally and not giving more importance to some nodes and ignoring others. <a href=\"https://keras.io/api/layers/normalization_layers/batch_normalization/\">Normalization</a> does a similar work by normalizing the inputs from the previous\u00a0layer.</p>\n<p><strong>1.4 Flatten</strong></p>\n<p>Flatten just makes the multi dimensional array to single dimensioned.</p>\n<h4>Augment the\u00a0data</h4>\n<p>Augmentation is necessary to make sure that the model is not just memorizing data and is generalizing. The different kind of augmentations are: zooming the image, x variation, y variation, etc. You can read more augmentations <a href=\"https://keras.io/api/preprocessing/image/\">here</a>. In part 6, I describe augmentation more in detail as we create our\u00a0own.</p>\n<h4><strong>Compiling the model and defining the loss and optimizer</strong></h4>\n<p>These are parameters to the model. The loss defines the difference between the actual and prediction which needs to be backpropagated. We have discussed in the start of the\u00a0article.</p>\n<h4><strong>Training the\u00a0model</strong></h4>\n<p>Here you can change the batch size. The batch is the number of images the loss will be accumulated over before backpropagating. With the above steps that is shuffling the images, augmenting the images and adding Dropout and Batch Normalization to the model, this last steps also ensures generalization of the\u00a0model.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8e69489c860d\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>My 3 months with Computer Vision\u200a\u2014\u200aPart 2\u200a\u2014\u200aUnderstanding Neural Network and\u00a0Layers</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/236/1*Zz19MwrI4-gzYNoEoxJvnQ.png\"><figcaption>Neural Network</figcaption></figure><h3><strong>What are Neural Networks?</strong></h3>\n<p><a href=\"https://en.wikipedia.org/wiki/Neural_network\">Neural Network</a> in very simple terms is a bunch of neural which perform transformations from one variable to other. These nodes are arranged together in the hidden layer. The values flow from the input to the output and the error that comes is then <a href=\"https://en.wikipedia.org/wiki/Backpropagation\">backpropagated</a> to the input layers. This is known as the Dense Layer and is the foundation of Neural Networks. So we already have 2 important parameters to understand for training a neural network\u00a0\u2014</p>\n<ol>\n<li>Error</li>\n<li>And how is this error backpropagated?</li>\n</ol>\n<h3><strong>What is Deep Learning?</strong></h3>\n<p>Deep Learning is just adding some layers before the dense layers which try to detect the features which can be trained. For example, before these layers were introduced, Software Engineers used to have to program features into the neural network. Now these can be auto detected using the following layers.</p>\n<h4><strong>Understanding Convolution</strong></h4>\n<p>Convolution means that the input starts large and multi dimensional and is reduced to smaller output. So you start by adding Convolutional layers and you keep reducing the size of the input to be processed every\u00a0layer.</p>\n<h4><strong>Computer Vision\u00a0Layers</strong></h4>\n<p><strong>1.1 Conv2D</strong></p>\n<p><a href=\"https://keras.io/api/layers/convolution_layers/convolution2d/\">Conv2D</a> is a list of filters which you pass over the image or the image array and that filtered output is now the input to the next layer. These filters are slid over the array and multiplied with the layer below. You can read about the parameters you can pass in the <a href=\"https://keras.io/api/layers/convolution_layers/convolution2d/\">Keras</a> documentation.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/500/0*JS_gxiQjnpPQvIaV.gif\"><figcaption>Convolution Visualized</figcaption></figure><p><strong>1.2 MaxPooling2D</strong></p>\n<p><a href=\"https://keras.io/api/layers/pooling_layers/max_pooling2d/\">MaxPooling2D</a> is a pooling layer and is much simpler to explain. All this does is given height and width, find the max value from that array. The filter is slid over the beneath layer just like\u00a0before.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/728/0*ETjRQFIb45O-MLw4.gif\"><figcaption>MaxPooling2D Visualized</figcaption></figure><p><strong>1.3 Normalizing &amp; Regularization Layers</strong></p>\n<p>We need these layers to keep the model robust to change. The model might get into a wrong tangent and think that some parameter is really important. <a href=\"https://keras.io/api/layers/regularization_layers/dropout/\">Dropout</a> drops some nodes at random. This makes sure that the model is learning all nodes equally and not giving more importance to some nodes and ignoring others. <a href=\"https://keras.io/api/layers/normalization_layers/batch_normalization/\">Normalization</a> does a similar work by normalizing the inputs from the previous\u00a0layer.</p>\n<p><strong>1.4 Flatten</strong></p>\n<p>Flatten just makes the multi dimensional array to single dimensioned.</p>\n<h4>Augment the\u00a0data</h4>\n<p>Augmentation is necessary to make sure that the model is not just memorizing data and is generalizing. The different kind of augmentations are: zooming the image, x variation, y variation, etc. You can read more augmentations <a href=\"https://keras.io/api/preprocessing/image/\">here</a>. In part 6, I describe augmentation more in detail as we create our\u00a0own.</p>\n<h4><strong>Compiling the model and defining the loss and optimizer</strong></h4>\n<p>These are parameters to the model. The loss defines the difference between the actual and prediction which needs to be backpropagated. We have discussed in the start of the\u00a0article.</p>\n<h4><strong>Training the\u00a0model</strong></h4>\n<p>Here you can change the batch size. The batch is the number of images the loss will be accumulated over before backpropagating. With the above steps that is shuffling the images, augmenting the images and adding Dropout and Batch Normalization to the model, this last steps also ensures generalization of the\u00a0model.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8e69489c860d\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["computer-vision","keras","data-augmentation","deep-learning","tensorflow"]}]}